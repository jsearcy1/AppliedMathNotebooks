{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will calculate the bias/variance tradeoff using polynomial regression.\n",
    "\n",
    "Our dataset consists of a independent variable $x$ and a dependent variable $y$ (targets), related by our ground truth model $y=f(x)$. We will choose a specific model $f(x)=0.5x+x^2$, and generate $i=1,\\ldots,N$ observations using the following generative model\n",
    "$$\n",
    "y_i=f(x_i)+\\epsilon_i\n",
    "$$\n",
    "where the irreducible or measurement noise is sampled from a gaussian distribution\n",
    "$$\n",
    "\\epsilon_i\\sim {\\cal N}(0,\\sigma_\\epsilon^2)\n",
    "$$\n",
    "We generate a training set with $N$ observations uniformly distributed along the interval $x\\in[-x_{max},x_{max}]$, with $x_{max}=1$ and a test set consisting of a single point $(x_{test},y_{test})$, with $x_{test}=1.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ground truth model f(x)\n",
    "\n",
    "# full dataset (x,y) set consists of N observations in the interval [-x_max,x_max]\n",
    "# we will subsample from this to create different training sets\n",
    "\n",
    "# measurement noise\n",
    "\n",
    "# full dataset set (x,y)\n",
    "\n",
    "\n",
    "# test set outside of training range:\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_range = np.linspace(-x_max, x_max, 1000)\n",
    "# ground truth model\n",
    "plt.plot(x_range, f(x_range), 'g', linewidth=3.0)\n",
    "# training set\n",
    "plt.scatter(x, y)\n",
    "# test set\n",
    "plt.scatter(x_test, y_test, c='r')\n",
    "#\n",
    "plt.xlabel('x', size=12)\n",
    "plt.ylabel('y', size=12)\n",
    "plt.xticks(np.arange(-x_max, x_max + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to learn a function $\\hat{f}(x)$ that best approximates our ground truth model $f(x)$ from the noisy observations in the training set, and accurately predict the target in the test set.\n",
    "\n",
    "We will compare several alternative models, within the class of polynomials of degree $d$:\n",
    "$$\n",
    "\\hat f_d(x|w)=\\sum_{i=0}^d w^{(d)}_i x^i\n",
    "$$\n",
    "where $w^{(d)}_i$ are the parameters to be learned using polynomial regression (i.e., maximizing the likelihood, or equivalently minimizing the squared error of the training set), via the function np.polyfit. We will pick $d=0,\\ldots,5$. \n",
    "\n",
    "We will perform $a=1,\\ldots,n_{exp}=6$ experiments. For each experiment, we will create a new training set $S_{train}^a=[x_i,y_i]$ by randomly subsampling $0.5\\%$ of the original $N$ observations, and fitting each model $\\hat f_d(x|w)$ to the training set $S_{train}^a$, to learn the parameters $w^{(d)}_i(S_{train}^a)$. For each learned model $w^{(d)}_i(S_{train}^a)$, we will then make a prediction for the test set $y_{test}=\\hat f(x_{test}|w^{(d)}_i(S_{train}^a)$. Crucially, the learned parameters $w^{(d)}_i(S_{train}^a)$ and thus the prediction $y_{test}$ will be different in the different experiments: they are functions of the training set  $S_{train}^a$. We will thus plot the learned models $y=\\hat f(x|w^{(d)}_i(S_{train}^a)$ for all training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative models: polynomials of degree d in d_arr\n",
    "d_arr=[0,1,2,3,4,5]\n",
    "\n",
    "#our class of models to fit: polynomials of degree d\n",
    "def f_hat(x, w):\n",
    "    d = len(w) - 1\n",
    "    return np.sum(w * np.power(x, np.expand_dims(np.arange(d, -1, -1), 1)).T, 1)\n",
    "\n",
    "# number of experiments to perform\n",
    "\n",
    "# training data in each experiment\n",
    "\n",
    "# indices for sampling disjoint training sets for each experiment from the data above\n",
    "    \n",
    "cnt = 0\n",
    "colors = np.array(['tab:green', 'tab:purple', 'tab:cyan', 'tab:orange', 'tab:brown', 'tab:pink'])\n",
    "fig, axs = plt.subplots(2, 3, sharey=True, figsize=(15, 9))\n",
    "# store all polynomial fits for each experiment\n",
    "wtot=[] \n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        # creating one training set\n",
    "\n",
    "        # append polynomial fits    \n",
    "\n",
    "        # plot training data\n",
    "        axs[i, j].scatter(x_train, y_train)\n",
    "        # plot ground truth\n",
    "        axs[i, j].plot(x_range, f(x_range), 'r', linewidth=3.0,linestyle='--')\n",
    "        # plot model fits\n",
    "        for k in range(len(w)):\n",
    "            axs[i, j].plot(x_range, f_hat(x_range, w[k]), colors[k], linewidth=3.0)\n",
    "        # plot test data    \n",
    "        axs[i, j].scatter(x_test, y_test, c='r')\n",
    "        # plot prediction of test data using polynomial fits    \n",
    "        for k in range(len(w)):\n",
    "            axs[i, j].scatter(x_test, f_hat(x_test, w[k]), c=colors[k],marker='v')\n",
    "                \n",
    "        axs[i, j].set_xlabel('x', size=12)\n",
    "        axs[i, j].set_ylabel('y', size=12)\n",
    "        axs[i, j].legend([r'$f$', r'$\\hat{f}$ (d = 0)', r'$\\hat{f}$ (d = 1)', \n",
    "                          r'$\\hat{f}$ (d = 2)', r'$\\hat{f}$ (d = 3)'], fontsize=12)\n",
    "        axs[i, j].title.set_text('experiment {}'.format(cnt+1))\n",
    "        cnt += 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# visualize model fits to different experiments: variance\n",
    "\n",
    "fig, axs = plt.subplots(1,len(d_arr), sharey=True, figsize=(15, 4))\n",
    "for i in range(len(d_arr)):\n",
    "    for k in range(len(wtot)):\n",
    "        axs[i].plot(x_range, f_hat(x_range, wtot[k][i]), colors[i], linewidth=2.0)\n",
    "        axs[i].title.set_text('variance for ' r'$\\hat f$ (d = {})'.format(d_arr[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with low complexity, such as a horizontal line, are more consistent when learned from different training sets (low variance), while models with many parameters show more variability (high variance). \n",
    "\n",
    "We will estimate the **bias** of each model class as its average test set prediction (averaged over all different training sets) minus the ground truth:\n",
    "$$\n",
    "bias[\\hat f_d(x_{test})]=\\mathbb{E}_{exp}[ \\hat f_d(x_{test}|w^{(d)}_i)]-f(x_{test})\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathbb{E}_{exp}[ \\hat f_d(x|w^{(d)}_i)]={1\\over n_{exp}}\\sum_{a=1}^{n_{exp}}\\hat f_d(x|w^{(d)}_i(S_{train}^a))\n",
    "$$\n",
    "\n",
    "We will estimate the **variance** of each model class as the mean square deviation of the test prediction from the its average test set prediction (averaged over all different training sets):\n",
    "$$\n",
    "var[\\hat f_d(x_{test})]=\\mathbb{E}_{exp}[ \\left(\\hat f_d(x_{test}|w^{(d)}_i)-\\mathbb{E}_{exp}[\\hat f_d(x_{test}|w^{(d)}_i)]\\right)^2]\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathbb{E}_{exp}[ \\hat f_d(x|w^{(d)}_i)]={1\\over n_{exp}}\\sum_{a=1}^{n_{exp}}\\hat f_d(x|w^{(d)}_i(S_{train}^a))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of experiments (different training sets sampled from the population)\n",
    "\n",
    "# store predictions for test data point\n",
    "\n",
    "\n",
    "    # generate training sets\n",
    "        # fit training set with different models   \n",
    "\n",
    "# expected value of prediction for each model\n",
    "\n",
    "# variance of prediction across experiments\n",
    "\n",
    "\n",
    "# bias squared\n",
    "\n",
    "\n",
    "nplots=len(d_arr)\n",
    "fig, axs = plt.subplots(nplots, 1, sharex=True, sharey=True, figsize=(12, 3*nplots))\n",
    "ylim=np.percentile(y_hat_test.flatten(),[1,99]) # find percentiles for setting plot limits\n",
    "for k in range(nplots):\n",
    "    # distribution of predictions across training sets\n",
    "    axs[k].hist(y_hat_test[k], density=True, color=colors[k], alpha=0.6)             \n",
    "    xlim = axs[k].get_xlim()\n",
    "    # true value of test data \n",
    "    axs[k].plot([f(x_test), f(x_test)], [0, 1], 'r', linewidth=3.0)\n",
    "    # expected value of test prediction\n",
    "    axs[k].plot([y_hat_test_mean[k], y_hat_test_mean[k]], [0, 1], c='k', linewidth=3.0)\n",
    "    axs[k].title.set_text('d = {}, variance = {:.3f}, bias^2 = {:.3f}'.format(d_arr[k],variance1[k],bias_squared[k]))\n",
    "    axs[k].legend([r'$f(x_{test})$', r'$\\mathbb{E}[\\hat{f}(x_{test})]$', r'$\\hat{f}(x_{test})$'], fontsize=12)\n",
    "    axs[k].set_xlim(ylim)\n",
    "# overlay gaussian distribution of predictions with expected mean and variance\n",
    "for k in range(nplots):\n",
    "    x_range = np.linspace(xlim[0], xlim[1], 1000)\n",
    "    axs[k].plot(x_range, stats.norm.pdf(x_range, y_hat_test_mean[k], y_hat_test_std[k]), color=colors[k], ls='--')\n",
    "    \n",
    "plt.suptitle(r'Histogram of $\\hat{f}(x_{test})$', size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the bias and variance as a function of the degree of the learned polynomial, we find that:\n",
    "- Models with low complexity have high bias and low variance\n",
    "- Models with high complexity have low bias and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bias squared and variance from above\n",
    "# bias squared\n",
    "bias_squared = (np.mean(y_hat_test, 1) - f(x_test)) ** 2\n",
    "# variance\n",
    "var_y_hat_test = np.var(y_hat_test, 1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(d_arr, bias_squared, 'y--')\n",
    "plt.plot(d_arr, var_y_hat_test, 'b--')\n",
    "plt.plot(d_arr, bias_squared+var_y_hat_test+sigma_epsilon**2, 'r--')\n",
    "plt.xticks(d_arr)\n",
    "plt.xlabel('d=degree of polynomial', size=12)\n",
    "plt.ylabel('squared error', size=12)\n",
    "plt.legend([r'bias squared: $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$',\n",
    "            r'$var(\\hat{f}(x))$'], loc='upper center', fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
